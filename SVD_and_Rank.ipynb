{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00b6fb8-3efc-4145-9723-ca6b4ff65b33",
   "metadata": {},
   "source": [
    "### Import PyTorch and numpy libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e80cde-9699-44d3-adb6-02ee2fe4e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c797f4-8149-49b1-bf1f-d90a4f758c59",
   "metadata": {},
   "source": [
    "Let's create a Weight matrix W as a product of two matrices of shape (d, 2) (2, k). The rank of the matrix W will be 2. \n",
    "\n",
    "[More about Rank of a matrix](https://en.wikipedia.org/wiki/Rank_(linear_algebra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1465065-fb79-4335-8009-892f74048c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix W: torch.Size([10, 10])\n",
      "Rank of W: 2\n"
     ]
    }
   ],
   "source": [
    "d, k = 10, 10\n",
    "\n",
    "rank = 2\n",
    "W = torch.randn(d, rank) @ torch.randn(rank, k)\n",
    "print('Shape of matrix W:', W.shape)\n",
    "W_rank = np.linalg.matrix_rank(W)\n",
    "print(f'Rank of W: {W_rank}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c619d-11a7-4c4d-8c16-723a877221da",
   "metadata": {},
   "source": [
    "To understand how can we decompose the matrix W to matrices with lower rank, \n",
    "which means that we need lesser number of parameters to represent W\n",
    "\n",
    "Lets apply SVD on W. More about SVD here: [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548b058e-8be8-4bf2-9dd0-3d7361079227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10]) torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "U, S, V = torch.svd(W)\n",
    "\n",
    "# Perform SVD on W (W = UxSxV^T)\n",
    "\n",
    "print(U.shape, S.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d45259-a8fc-44bb-b347-31e0de45d42c",
   "metadata": {},
   "source": [
    "### For rank-r factorization, keep only the first r singular values (and corresponding columns of U and V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c739fced-3588-4911-a426-ea16f76383a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_r = U[:, :W_rank]\n",
    "S_r = torch.diag(S[:W_rank])\n",
    "V_r = V[:, :W_rank].t()  # Transpose V_r to get the right dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f450370d-4de8-47ec-97eb-8f0577f3c8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of B: torch.Size([10, 2])\n",
      "Shape of A: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Compute B = U_r * S_r and A = V_r\n",
    "B = U_r @ S_r\n",
    "A = V_r\n",
    "print(f'Shape of B: {B.shape}')\n",
    "print(f'Shape of A: {A.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8c8fe-a9fd-4113-99d5-be6670ab3732",
   "metadata": {},
   "source": [
    "Now we got B and A, thourgh the singular value decomposition of matrix W. \n",
    "\n",
    "Let's perform simple linear regression with randomly generated  input(x) and bias(b) vectors and the weight matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e76e47a8-0727-440c-8b0d-67a56661678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random bias and input\n",
    "bias = torch.randn(d)\n",
    "x = torch.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede4548e-1c20-47dc-a536-b1cda74bda2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y using W:\n",
      " tensor([ 0.9137, -1.9848,  1.8892,  2.4349, -2.2102, -2.1251,  1.1640,  2.1034,\n",
      "         0.2520, -2.4652])\n",
      "y' computed using BA:\n",
      " tensor([ 0.9137, -1.9848,  1.8892,  2.4349, -2.2102, -2.1251,  1.1640,  2.1034,\n",
      "         0.2520, -2.4652])\n"
     ]
    }
   ],
   "source": [
    "# Compute y = Wx + bias\n",
    "y = W @ x + bias\n",
    "# Compute y' = (B*A)x + bias\n",
    "y_prime = (B @ A) @ x + bias\n",
    "\n",
    "print(\"Original y using W:\\n\", y)\n",
    "print(\"y' computed using BA:\\n\", y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff2dc82-453a-4073-9a9e-9be9117dbeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters of W:  100\n",
      "Total parameters of B and A:  40\n"
     ]
    }
   ],
   "source": [
    "print(\"Total parameters of W: \", W.nelement())\n",
    "print(\"Total parameters of B and A: \", B.nelement() + A.nelement())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e82ef-bb2d-4be0-91aa-060b14846830",
   "metadata": {},
   "source": [
    "Instead of 100 parameters in W, we can represent W using BA using only 40 parameters (We can infer this as y==y').\n",
    "Now that we have an idea of matrix decomposition, Let's proceed to LoRA, a trainable way to introduce rank decomposition matrices into each\r\n",
    "layer of theneural network architectures. \n",
    "\n",
    "[LoRA paper](https://arxiv.org/pdf/2106.09685.pdf)e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
